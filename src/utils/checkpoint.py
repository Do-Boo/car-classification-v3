#!/usr/bin/env python3
"""
π€ SOTA μ•™μƒλΈ” μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬ μ ν‹Έλ¦¬ν‹°
κ³ μ„±λ¥ ν•™μµμ„ μ„ν• μ²΄ν¬ν¬μΈνΈ μ €μ¥/λ΅λ“/κ΄€λ¦¬ μ‹μ¤ν…

μƒμ„±μΌ: 2025-06-02 17:04
λ©ν‘: μ•μ •μ μΈ μ¥μ‹κ°„ ν•™μµ μ§€μ›
"""

import torch
import json
import glob
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
import warnings
warnings.filterwarnings('ignore')

def save_checkpoint(model, optimizer, scheduler, epoch, metrics, filepath, **kwargs):
    """
    π”„ SOTA μ²΄ν¬ν¬μΈνΈ μ €μ¥ (μ™„μ „ν• ν•™μµ μƒνƒ λ³΄μ΅΄)
    
    Args:
        model: PyTorch λ¨λΈ
        optimizer: μµν‹°λ§μ΄μ €
        scheduler: ν•™μµλ¥  μ¤μΌ€μ¤„λ¬
        epoch: ν„μ¬ μ—ν¬ν¬
        metrics: μ„±λ¥ λ©”νΈλ¦­ λ”•μ…”λ„λ¦¬
        filepath: μ €μ¥ κ²½λ΅
        **kwargs: μ¶”κ°€ μ •λ³΄ (config, model_name λ“±)
    """
    # λ””λ ‰ν† λ¦¬ μƒμ„±
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'metrics': metrics,
        'model_name': model.__class__.__name__,
        'timestamp': str(Path(__file__).stat().st_mtime),  # μ €μ¥ μ‹κ°„
    }
    
    # μ¶”κ°€ μ •λ³΄ μ €μ¥
    checkpoint.update(kwargs)
    
    # μ²΄ν¬ν¬μΈνΈ μ €μ¥
    torch.save(checkpoint, filepath)
    print(f"π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥: {filepath}")
    print(f"   π“ Epoch: {epoch}")
    print(f"   π― Metrics: {metrics}")
    
    return checkpoint

def load_checkpoint(filepath, model, optimizer=None, scheduler=None, device='cpu'):
    """
    π“‚ SOTA μ²΄ν¬ν¬μΈνΈ λ΅λ“ (μ™„μ „ν• ν•™μµ μƒνƒ λ³µμ›)
    
    Args:
        filepath: μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅
        model: PyTorch λ¨λΈ
        optimizer: μµν‹°λ§μ΄μ € (μ„ νƒμ )
        scheduler: μ¤μΌ€μ¤„λ¬ (μ„ νƒμ )
        device: λ””λ°”μ΄μ¤
        
    Returns:
        tuple: (model, optimizer, scheduler, start_epoch)
    """
    if not Path(filepath).exists():
        raise FileNotFoundError(f"β μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {filepath}")
    
    print(f"π“‚ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ¤‘: {filepath}")
    checkpoint = torch.load(filepath, map_location=device)
    
    # λ¨λΈ μƒνƒ λ΅λ“
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"β… λ¨λΈ μƒνƒ λ΅λ“ μ™„λ£")
    
    # μµν‹°λ§μ΄μ € μƒνƒ λ΅λ“
    start_epoch = checkpoint.get('epoch', 0)
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"β… μµν‹°λ§μ΄μ € μƒνƒ λ΅λ“ μ™„λ£")
    
    # μ¤μΌ€μ¤„λ¬ μƒνƒ λ΅λ“
    if scheduler is not None and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"β… μ¤μΌ€μ¤„λ¬ μƒνƒ λ΅λ“ μ™„λ£")
    
    # λ©”νΈλ¦­ μ •λ³΄ μ¶λ ¥
    metrics = checkpoint.get('metrics', {})
    print(f"π“ μ΄μ „ μ„±λ¥:")
    for key, value in metrics.items():
        print(f"   β€Ά {key}: {value}")
    
    print(f"π”„ Epoch {start_epoch}λ¶€ν„° ν•™μµ μ¬κ°")
    
    return model, optimizer, scheduler, start_epoch

def find_last_checkpoint(save_dir: Path, fold: int) -> Tuple[Optional[str], int]:
    """
    π” λ§μ§€λ§‰ μ²΄ν¬ν¬μΈνΈ μλ™ νƒμ§€ (μ¤‘λ‹¨λ ν•™μµ μλ™ μ¬κ°)
    
    Args:
        save_dir: μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ””λ ‰ν† λ¦¬
        fold: K-Fold λ²νΈ
        
    Returns:
        tuple: (checkpoint_path, start_epoch)
    """
    save_dir = Path(save_dir)
    
    # κ°€λ¥ν• μ²΄ν¬ν¬μΈνΈ ν¨ν„΄λ“¤
    patterns = [
        f"best_fold_{fold}.pth",           # μµκ³  μ„±λ¥ λ¨λΈ
        f"checkpoint_fold_{fold}_*.pth",   # μ—ν¬ν¬λ³„ μ²΄ν¬ν¬μΈνΈ
        f"last_fold_{fold}.pth",           # λ§μ§€λ§‰ μ²΄ν¬ν¬μΈνΈ
    ]
    
    checkpoint_files = []
    
    # λ¨λ“  ν¨ν„΄μΌλ΅ νμΌ κ²€μƒ‰
    for pattern in patterns:
        files = list(save_dir.glob(pattern))
        checkpoint_files.extend(files)
    
    if not checkpoint_files:
        print(f"π“ μƒλ΅μ΄ ν•™μµ μ‹μ‘ (Fold {fold})")
        return None, 0
    
    # κ°€μ¥ μµκ·Ό νμΌ μ„ νƒ (μμ • μ‹κ°„ κΈ°μ¤€)
    latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
    
    try:
        # μ²΄ν¬ν¬μΈνΈμ—μ„ μ—ν¬ν¬ μ •λ³΄ μ¶”μ¶
        checkpoint = torch.load(latest_checkpoint, map_location='cpu')
        start_epoch = checkpoint.get('epoch', 0)
        
        print(f"π”„ μ²΄ν¬ν¬μΈνΈ λ°κ²¬: {latest_checkpoint}")
        print(f"π“ Epoch {start_epoch}λ¶€ν„° μ¬κ°")
        
        return str(latest_checkpoint), start_epoch
        
    except Exception as e:
        print(f"β οΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}")
        print(f"π“ μƒλ΅μ΄ ν•™μµ μ‹μ‘")
        return None, 0

def cleanup_old_checkpoints(save_dir: Path, fold: int, keep_best: bool = True, keep_last: int = 3):
    """
    π§Ή μ¤λλ μ²΄ν¬ν¬μΈνΈ μ •λ¦¬ (λ””μ¤ν¬ κ³µκ°„ μ μ•½)
    
    Args:
        save_dir: μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬
        fold: K-Fold λ²νΈ
        keep_best: μµκ³  μ„±λ¥ λ¨λΈ μ μ§€ μ—¬λ¶€
        keep_last: μ μ§€ν•  μµκ·Ό μ²΄ν¬ν¬μΈνΈ μ
    """
    save_dir = Path(save_dir)
    
    # μ—ν¬ν¬λ³„ μ²΄ν¬ν¬μΈνΈ νμΌλ“¤ μ°ΎκΈ°
    pattern = f"checkpoint_fold_{fold}_epoch_*.pth"
    checkpoint_files = list(save_dir.glob(pattern))
    
    if len(checkpoint_files) <= keep_last:
        return  # μ •λ¦¬ν•  νμΌμ΄ μ¶©λ¶„ν•μ§€ μ•μ
    
    # μμ • μ‹κ°„ κΈ°μ¤€μΌλ΅ μ •λ ¬ (μµμ‹  μ)
    checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    
    # μ‚­μ ν•  νμΌλ“¤ (μµκ·Ό Nκ° μ μ™Έ)
    files_to_delete = checkpoint_files[keep_last:]
    
    for file_path in files_to_delete:
        try:
            file_path.unlink()
            print(f"π—‘οΈ μ¤λλ μ²΄ν¬ν¬μΈνΈ μ‚­μ : {file_path.name}")
        except Exception as e:
            print(f"β οΈ νμΌ μ‚­μ  μ‹¤ν¨: {file_path.name} - {e}")
    
    print(f"β… μ²΄ν¬ν¬μΈνΈ μ •λ¦¬ μ™„λ£ (μµκ·Ό {keep_last}κ° μ μ§€)")

def save_ensemble_checkpoint(ensemble_results: Dict[str, Any], fold: int, output_dir: str = "outputs/ensemble"):
    """
    π† μ•™μƒλΈ” κ²°κ³Ό μ²΄ν¬ν¬μΈνΈ μ €μ¥
    
    Args:
        ensemble_results: μ•™μƒλΈ” λ¨λΈ κ²°κ³Ό λ”•μ…”λ„λ¦¬
        fold: K-Fold λ²νΈ
        output_dir: μ¶λ ¥ λ””λ ‰ν† λ¦¬
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # JSON ν•νƒλ΅ μ•™μƒλΈ” κ²°κ³Ό μ €μ¥
    results_path = output_dir / f"ensemble_results_fold_{fold}.json"
    
    # JSON μ§λ ¬ν™” κ°€λ¥ν• ν•νƒλ΅ λ³€ν™
    serializable_results = {}
    for model_name, result in ensemble_results.items():
        serializable_results[model_name] = {
            'model_name': result['model_name'],
            'model_path': result['model_path'],
            'val_loss': float(result['val_loss']),
            'weight': float(result['weight']),
            'description': result['description'],
            # configλ” λ„λ¬΄ ν¬λ―€λ΅ μ μ™Έ
        }
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    print(f"π† μ•™μƒλΈ” κ²°κ³Ό μ €μ¥: {results_path}")
    print(f"π“ λ¨λΈ μ: {len(ensemble_results)}")
    
    return results_path

def load_ensemble_checkpoint(fold: int, output_dir: str = "outputs/ensemble") -> Optional[Dict[str, Any]]:
    """
    π“‚ μ•™μƒλΈ” κ²°κ³Ό μ²΄ν¬ν¬μΈνΈ λ΅λ“
    
    Args:
        fold: K-Fold λ²νΈ
        output_dir: μ¶λ ¥ λ””λ ‰ν† λ¦¬
        
    Returns:
        dict: μ•™μƒλΈ” κ²°κ³Ό λ”•μ…”λ„λ¦¬ λλ” None
    """
    results_path = Path(output_dir) / f"ensemble_results_fold_{fold}.json"
    
    if not results_path.exists():
        print(f"π“ μ•™μƒλΈ” κ²°κ³Ό νμΌμ΄ μ—†μµλ‹λ‹¤: {results_path}")
        return None
    
    try:
        with open(results_path, 'r', encoding='utf-8') as f:
            ensemble_results = json.load(f)
        
        print(f"π“‚ μ•™μƒλΈ” κ²°κ³Ό λ΅λ“: {results_path}")
        print(f"π“ λ¨λΈ μ: {len(ensemble_results)}")
        
        return ensemble_results
        
    except Exception as e:
        print(f"β μ•™μƒλΈ” κ²°κ³Ό λ΅λ“ μ‹¤ν¨: {e}")
        return None

def get_checkpoint_info(checkpoint_path: str) -> Dict[str, Any]:
    """
    π“ μ²΄ν¬ν¬μΈνΈ μ •λ³΄ μ΅°ν
    
    Args:
        checkpoint_path: μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅
        
    Returns:
        dict: μ²΄ν¬ν¬μΈνΈ μ •λ³΄
    """
    if not Path(checkpoint_path).exists():
        return {"error": "νμΌμ΄ μ΅΄μ¬ν•μ§€ μ•μµλ‹λ‹¤"}
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        info = {
            "epoch": checkpoint.get('epoch', 'Unknown'),
            "model_name": checkpoint.get('model_name', 'Unknown'),
            "metrics": checkpoint.get('metrics', {}),
            "file_size": f"{Path(checkpoint_path).stat().st_size / (1024*1024):.1f} MB",
            "timestamp": checkpoint.get('timestamp', 'Unknown'),
        }
        
        return info
        
    except Exception as e:
        return {"error": f"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}"}

if __name__ == "__main__":
    # ν…μ¤νΈ μ½”λ“
    print("π§ μ²΄ν¬ν¬μΈνΈ μ ν‹Έλ¦¬ν‹° ν…μ¤νΈ")
    
    # ν…μ¤νΈ λ””λ ‰ν† λ¦¬
    test_dir = Path("test_checkpoints")
    test_dir.mkdir(exist_ok=True)
    
    # find_last_checkpoint ν…μ¤νΈ
    checkpoint_path, start_epoch = find_last_checkpoint(test_dir, fold=0)
    print(f"κ²°κ³Ό: {checkpoint_path}, {start_epoch}")
    
    # ν…μ¤νΈ λ””λ ‰ν† λ¦¬ μ •λ¦¬
    import shutil
    shutil.rmtree(test_dir, ignore_errors=True)
    
    print("β… ν…μ¤νΈ μ™„λ£!") 