"""
í•™ìŠµì— í•„ìš”í•œ í—¬í¼ í•¨ìˆ˜ë“¤
"""

import torch
import numpy as np
from pathlib import Path

class EarlyStopping:
    """Early Stopping í´ë˜ìŠ¤"""
    
    def __init__(self, patience=10, mode='max', delta=0.001, verbose=True):
        """
        Args:
            patience: ê°œì„ ì´ ì—†ì„ ë•Œ ê¸°ë‹¤ë¦´ ì—í­ ìˆ˜
            mode: 'max' (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) ë˜ëŠ” 'min' (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            delta: ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰
            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.patience = patience
        self.mode = mode
        self.delta = delta
        self.verbose = verbose
        
        self.best_score = None
        self.counter = 0
        self.early_stop = False
        
        if mode == 'max':
            self.is_better = lambda score, best: score > best + delta
        else:
            self.is_better = lambda score, best: score < best - delta
    
    def __call__(self, score):
        """
        Args:
            score: í˜„ì¬ ì ìˆ˜
            
        Returns:
            bool: Early stopping ì—¬ë¶€
        """
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.is_better(score, self.best_score):
            self.best_score = score
            self.counter = 0
            if self.verbose:
                print(f"âœ… ì„±ëŠ¥ ê°œì„ ! Best score: {self.best_score:.6f}")
        else:
            self.counter += 1
            if self.verbose:
                print(f"â³ ê°œì„  ì—†ìŒ ({self.counter}/{self.patience})")
            
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"ğŸ›‘ Early stopping triggered!")
        
        return self.early_stop

def save_checkpoint(model, optimizer, epoch, score, filepath, scheduler=None, **kwargs):
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    
    Args:
        model: PyTorch ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì €
        epoch: í˜„ì¬ ì—í­
        score: í˜„ì¬ ì ìˆ˜
        filepath: ì €ì¥ ê²½ë¡œ
        scheduler: ìŠ¤ì¼€ì¤„ëŸ¬ (ì„ íƒì )
        **kwargs: ì¶”ê°€ ì •ë³´
    """
    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'score': score,
        'model_name': model.__class__.__name__,
    }
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì €ì¥
    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    # ì¶”ê°€ ì •ë³´ ì €ì¥
    checkpoint.update(kwargs)
    
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")

def load_checkpoint(filepath, model, optimizer=None, scheduler=None, device='cpu'):
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    
    Args:
        filepath: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        model: PyTorch ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì € (ì„ íƒì )
        scheduler: ìŠ¤ì¼€ì¤„ëŸ¬ (ì„ íƒì )
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        dict: ì²´í¬í¬ì¸íŠ¸ ì •ë³´
    """
    if not Path(filepath).exists():
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
    
    checkpoint = torch.load(filepath, map_location=device)
    
    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ
    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {filepath}")
    print(f"   - Epoch: {checkpoint.get('epoch', 'Unknown')}")
    print(f"   - Score: {checkpoint.get('score', 'Unknown')}")
    
    return checkpoint

def set_seed(seed=42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    import random
    import os
    
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")

def count_parameters(model):
    """ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
    print(f"   - ì „ì²´: {total_params:,}")
    print(f"   - í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}")
    
    return total_params, trainable_params

def get_lr(optimizer):
    """í˜„ì¬ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°"""
    for param_group in optimizer.param_groups:
        return param_group['lr']

def save_predictions(predictions, ids, filepath, class_names=None):
    """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
    import pandas as pd
    
    # DataFrame ìƒì„±
    if class_names is not None:
        df = pd.DataFrame(predictions, columns=class_names)
        df.insert(0, 'ID', ids)
    else:
        df = pd.DataFrame({'ID': ids})
        for i in range(predictions.shape[1]):
            df[f'class_{i}'] = predictions[:, i]
    
    # ì €ì¥
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(filepath, index=False)
    print(f"ğŸ“„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {filepath}")
    
    return df 